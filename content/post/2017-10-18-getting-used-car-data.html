---
title: Webscraping Thousands of Used Cars
author: ~
date: '2017-10-18'
slug: getting-used-car-data
categories: ['tutorials']
tags: ['r', 'webscraping']
---



<p>In another post, I describe how I use this data that I’ve scraped, but I wanted to provide a more in-depth tutorial for those interested in how I got the data. Note, this data belongs to Truecar, so all uses herein are for personal and academic reasons only.</p>
<div id="get-the-data" class="section level1">
<h1>Get the data</h1>
<p>In order to do any good analaysis, you first need data. I prefer to have more data than less, where possible. In this case, I don’t have any data, so I use webscraping to get the data. There are much better tutorials on how to scrape data, so I’ll be light. I use R’s <code>rvest</code> package here, which does a decent job.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> Let’s look at Truecar’s Used Car postings<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>. First I use <code>google</code> to find the search query on Truecar that I like.</p>
<pre class="r"><code># Load packages
library(rvest)
library(dplyr)
library(magrittr)</code></pre>
<pre class="r"><code># Find the URL of the data you want to scrape
url &lt;- &#39;https://www.truecar.com/used-cars-for-sale/listings/ford/edge/&#39;
read_html(url)</code></pre>
<pre><code>## {xml_document}
## &lt;html lang=&quot;en-US&quot;&gt;
## [1] &lt;head&gt;\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset= ...
## [2] &lt;body&gt;\n    &lt;!-- Element target for any partner code meant to execut ...</code></pre>
<p>You’ll see there’s a head and a body. Our data’s in the body, so let’s use <code>html_nodes()</code> and <code>html_text()</code> to parse out the data we want. I used <a href="https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html">Selectorgadget</a> to know what HTML classes to search for.</p>
<pre class="r"><code>read_html(url) %&gt;% html_nodes(&#39;.col-xs-6.col-sm-8.no-left-padding&#39;) %&gt;% html_text()</code></pre>
<pre><code>## character(0)</code></pre>
<p>So that’s how you get the data on a single page. If you look closer at the URL, you see a lot of helpful things. First, there’s the make, then the model, then the location-zip, then the year-range, and ultimately the trim. This is a very pretty and clean URL. If you click on a few additional pages, you’ll see the URL opens up with <code>?page=2</code>.</p>
<pre><code>https://www.truecar.com/used-cars-for-sale/listings/ford/edge/location-90210/?page=2</code></pre>
<p>This is our ‘in’ to scraping multiple pages. I won’t bore you with the details of how to get <em>that</em> data into a neat matrix for us to analyze, but suffice it to say that I’m able to do it. Just build a function to construct a URL, and build a loop to go through the different pages, then use lots of <code>str_extract</code> from the <code>stringr</code> package and <code>gsub</code> to clean up the data.</p>
<pre class="r"><code>library(stringr)

make = &#39;ford&#39;
model = &#39;edge&#39;
zip = &#39;90210&#39;
year = 2012
npages = 5

url &lt;- paste(&#39;https://www.truecar.com/used-cars-for-sale/listings/&#39;, 
             make, &#39;/&#39;, 
             model ,
             &#39;/location-&#39;, zip,
             &#39;/year-&#39;,year,&#39;-max/?page=&#39;, sep = &quot;&quot;)

urls &lt;- paste(url, 1:npages, sep = &quot;&quot;)

scrape &lt;- function(pageno){
  try(
    read_html(urls[pageno]) %&gt;% html_nodes(&#39;.col-xs-6.col-sm-8.no-left-padding&#39;) %&gt;% html_text()
  )
}

long_list = scrape(1)
for(i in 2:npages){
  print(i)
  new_list = try(scrape(i))
  
  error = (&quot;try-error&quot; %in% class(new_list))
  
  if( error == FALSE ){
    long_list = c(long_list, new_list) 
  } else {
    break
  }
}</code></pre>
<pre><code>## [1] 2</code></pre>
<pre class="r"><code>stats &lt;- long_list
df &lt;- as.data.frame(stats)
df$stats %&lt;&gt;% as.character()
df$price &lt;- str_extract(df$stats, &#39;\\$[0-9]*,[0-9]*&#39;) %&gt;% 
  gsub(&#39;Price: |\\$|,&#39;, &#39;&#39;, .) %&gt;%
  as.numeric()
df$year &lt;- str_extract(df$stats, &#39;^[0-9]* &#39;) %&gt;% 
  as.numeric()
df$mileage &lt;- str_extract(df$stats, &#39;Mileage: [0-9]*,[0-9]*&#39;) %&gt;% 
  gsub(&#39;Mileage: |,&#39;, &#39;&#39;, .) %&gt;%
  as.numeric()

# a = df$stats[1]
df$trim &lt;- str_extract(df$stats, &#39;.*Mileage:&#39;) %&gt;% 
  gsub(&#39;FWD|AWD|4x[24]|[24]WD|V6|4-cyl|^[0-9][0-9][0-9][0-9]|4dr|Automatic|Manual|Mileage:&#39;, &#39;&#39;, ., ignore.case = T) %&gt;% 
  gsub(make, &#39;&#39;, ., ignore.case = T) %&gt;% 
  gsub(model, &#39;&#39;, ., ignore.case = T) %&gt;% 
  trimws() 


df$awd &lt;- grepl(&#39;AWD|4WD|4x4&#39;, df$stats, ignore.case = T) %&gt;% as.numeric()
df$manual &lt;- grepl(&#39;manual&#39;, df$stats) %&gt;% as.numeric()
df$v6 &lt;- grepl(&#39;V6&#39;, df$stats) %&gt;% as.numeric()
df$location &lt;- str_extract(df$stats, &#39;Location: .*Exterior:&#39;) %&gt;% 
  gsub(&#39;Location: |Exterior:&#39;, &#39;&#39;, .) %&gt;% 
  trimws() 
df$ext &lt;- str_extract(df$stats, &#39;Exterior: .*Interior:&#39;) %&gt;% 
  gsub(&#39;Interior:|Exterior:&#39;, &#39;&#39;, .) %&gt;% 
  trimws() 
df$int &lt;- str_extract(df$stats, &#39;Interior: .*VIN:&#39;) %&gt;% 
  gsub(&#39;Interior: |VIN:&#39;, &#39;&#39;, .) %&gt;% 
  trimws() 
df$vin &lt;- str_extract(df$stats, &#39;VIN: .*\\$&#39;) %&gt;% 
  gsub(&#39;VIN: |\\$&#39;, &#39;&#39;, .) %&gt;% 
  substr(., 1, 17)
df$deal &lt;- str_extract(df$stats, &#39;\\$[0-9]*,[0-9]* below&#39;) %&gt;% 
  gsub(&#39;below|\\$|,&#39;, &#39;&#39;, .) %&gt;% trimws() %&gt;%
  as.numeric()</code></pre>
<p>And here’s what the results look like. You’ve got the original scraped data in the <code>stats</code> column and then everything else that you can parse out. Just like that, you’ve got</p>
<pre class="r"><code># df was the dataframe object we needed
df %&gt;% select(-stats) %&gt;% head(10) %&gt;% formattable::formattable()</code></pre>
<table class="table table-condensed">
<thead>
<tr>
<th style="text-align:right;">
price
</th>
<th style="text-align:right;">
year
</th>
<th style="text-align:right;">
mileage
</th>
<th style="text-align:right;">
trim
</th>
<th style="text-align:right;">
awd
</th>
<th style="text-align:right;">
manual
</th>
<th style="text-align:right;">
v6
</th>
<th style="text-align:right;">
location
</th>
<th style="text-align:right;">
ext
</th>
<th style="text-align:right;">
int
</th>
<th style="text-align:right;">
vin
</th>
<th style="text-align:right;">
deal
</th>
</tr>
</thead>
<tbody>
<tr>
</tr>
</tbody>
</table>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Python’s <code>beautifulSoup</code> package could probably do a better job.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>I tried scraping CarGurus, but wasn’t able to paginate. I tried scraping CarMax, but had difficulty. Edmunds was also easy, but Truecar was easiset.<a href="#fnref2">↩</a></p></li>
</ol>
</div>
