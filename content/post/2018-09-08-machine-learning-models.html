---
title: Machine Learning Model Notes
author: ~
date: '2018-09-08'
slug: machine-learning-models
categories: ['notes']
tags: ['machine-learning']
draft: false
---



<p>Here are some notes on machine learning models.</p>
<div id="concepts-behind-decision-trees" class="section level2">
<h2>Concepts Behind Decision Trees</h2>
<ul>
<li>Bagging (boostrap aggregation): Randomly sample with replacement, and average the results.</li>
<li>Majority vote: The most commonly-occuring prediction.</li>
<li>Internal node: Where the splits occur.</li>
<li>Branches: Segments that connect the nodes.</li>
<li>Terminal node (leafs, regions): Where the observations end up. The average of the responses (or majority vote) is the prediction for future observations.</li>
<li>Gini index: where <span class="math inline">\(m\)</span> is the leaf and <span class="math inline">\(k\)</span> is the class (0 or 1 for binary classification, but can be extedned for multiple classes). <span class="math inline">\(\hat{p}_{mk}\)</span> is the proportion of observations in leaf <span class="math inline">\(m\)</span>, class <span class="math inline">\(k\)</span> that belong. Gini index will reduce if <span class="math inline">\(\hat{p}_{mk}\)</span> is close to 0 or 1. (Variance will be low). Worst-case variance = .25 (.5*.5). Best-case variance is 0 (all observations are of one class).</li>
</ul>
<p><span class="math display">\[
G = \sum_{k \in K} \hat{p}_{mk} (1-\hat{p}_{mk})
\]</span></p>
<ul>
<li>Node purity: A node is pure if most of the observations come from one class. Gini index is a measure of purity.</li>
</ul>
<pre class="r"><code>library(ggplot2)
df = data.frame(p = seq(0, 1, length=100))

ggplot(df) + 
  geom_point(aes(x = p, y = p*(1-p), color=&#39;blue&#39;)) + 
  geom_point(aes(x = p, y = -p*log(p)))</code></pre>
<pre><code>## Warning: Removed 1 rows containing missing values (geom_point).</code></pre>
<p><img src="/post/2018-09-08-machine-learning-models_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
</div>
<div id="linear-regression" class="section level2">
<h2>Linear regression</h2>
<p>Variance-bias: Low variance.</p>
</div>
<div id="random-forest" class="section level2">
<h2>Random forest</h2>
<p>Why? * Decision trees suffer from high variance</p>
<p>How: * Randomly samples columns too (to de-correlate the trees). * Uses Bagging. Takes <span class="math inline">\(b\)</span> bootstrapped samples, fits a tree on each one, and averages the results. * For classification, the <em>majority vote</em> is use.</p>
<p>Concepts:</p>
<ul>
<li>OOB (out-of-bag) observations are those <em>not</em> selected during boostrapping. (For each tree). Use these observations (about 1/3) to explore. Keep track of each observation and whether it was bagged in a given tree. You estimate the prediction error across all the trees in which an observation was out-of-bag.</li>
</ul>
<p>Questions I have:</p>
<ul>
<li>For classification, each leaf has <span class="math inline">\(n\)</span> observations, and the mean of those observations is the prediction. Since each tree is constructed differently, the algorithm likely has to yield a score fore each tree. So the average for each leaf depends on <span class="math inline">\(n\)</span>. My guess is that <em>majority vote</em> is taking the 0-1 prediction from each tree (which would be a majority vote within a leaf), and then take the majority vote across the <span class="math inline">\(b\)</span> trees.</li>
</ul>
</div>
